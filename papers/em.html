

<title>EM Gibbs paper</title>

<p>

<center>
On Approximate  Convergence Properties of the Gibbs Sampler
 </center> 
 

  <center> Sujit K. Sahu,  
Faculty of Mathematical Studies, University of Southampton, Highfield, 
SO17 1BJ, UK.  </center>

 Gareth O. Roberts,   
Department of Mathematics and Statistics, 
Lancaster University, Lancaster,  LA1 4YF,  UK.
     

<hr>

<h2>
This paper has now appeared as
</h2>
    

<li>
Sahu, S. K. and Roberts, G. O. (1999) On Convergence of the EM Algorithm
and the Gibbs Sampler. <i>Statistics and Computing. </i><b>9, </b>55--64.</li>

<h4>          
 SUMMARY 
 </h4>
</center>
<p>

 
In this article we investigate  the relationship 
between  the EM algorithm and the 
Gibbs sampler. We show that the  approximate
rate of convergence of the Gibbs sampler
by Gaussian approximation is  equal to that of the corresponding EM 
type algorithm.  This helps in implementing either of the algorithms as 
improvement strategies for one algorithm can be directly transported to 
the other. In particular, by running the EM algorithm we know approximately 
how many iterations are needed for convergence of the Gibbs sampler. 
We also obtain a result that under {\em certain} conditions, the EM algorithm 
used for finding the maximum likelihood estimates can be slower to converge 
than the corresponding Gibbs sampler for Bayesian inference.  
We illustrate our results in a number of realistic 
examples all based on the generalized linear mixed models. 

 




<hr>
<a href="../index.html" > Back to </a> my page. 
<address>
S.K.Sahu@maths.soton.ac.uk 
</address>
<p>






